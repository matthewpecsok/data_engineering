{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewpecsok/data_engineering/blob/main/tutorials/de_streaming_kafka_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upgCc3gXybsA"
      },
      "source": [
        "### Install the required kafka packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "48B9eAMMhAgw",
        "outputId": "e43212f8-3853-4a7e-d95d-b792e78c38cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.10/dist-packages (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install kafka-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjrZNJQRJP-U"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "m6KXZuTBWgRm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from datetime import datetime\n",
        "import time\n",
        "import threading\n",
        "import json\n",
        "from kafka import KafkaProducer\n",
        "from kafka.errors import KafkaError\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZmI7l_GykcW"
      },
      "source": [
        "## Download and setup Kafka and Zookeeper instances\n",
        "\n",
        "For demo purposes, the following instances are setup locally:\n",
        "\n",
        "- Kafka (Brokers: 127.0.0.1:9092)\n",
        "- Zookeeper (Node: 127.0.0.1:2181)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: untar the tgz file\n",
        "\n",
        "!tar -xvzf filename.tgz"
      ],
      "metadata": {
        "id": "SiNEadYxnv_l",
        "outputId": "fb039756-6581-425d-c377-fdaf009a917e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar (child): filename.tgz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YUj0878jPyz7",
        "outputId": "99ca3d82-1791-4e8c-991b-2c6e9e950a1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kafka_2.12-3.7.1/\n",
            "kafka_2.12-3.7.1/LICENSE\n",
            "kafka_2.12-3.7.1/NOTICE\n",
            "kafka_2.12-3.7.1/bin/\n",
            "kafka_2.12-3.7.1/bin/kafka-delete-records.sh\n",
            "kafka_2.12-3.7.1/bin/trogdor.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-jmx.sh\n",
            "kafka_2.12-3.7.1/bin/connect-mirror-maker.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-console-consumer.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-consumer-perf-test.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-log-dirs.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-metadata-quorum.sh\n",
            "kafka_2.12-3.7.1/bin/zookeeper-server-stop.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-verifiable-consumer.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-features.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-acls.sh\n",
            "kafka_2.12-3.7.1/bin/zookeeper-server-start.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-server-stop.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-configs.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-reassign-partitions.sh\n",
            "kafka_2.12-3.7.1/bin/connect-plugin-path.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-leader-election.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-producer-perf-test.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-transactions.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-topics.sh\n",
            "kafka_2.12-3.7.1/bin/connect-standalone.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-e2e-latency.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-metadata-shell.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-get-offsets.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-dump-log.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-broker-api-versions.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-consumer-groups.sh\n",
            "kafka_2.12-3.7.1/bin/connect-distributed.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-delegation-tokens.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-run-class.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-replica-verification.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-console-producer.sh\n",
            "kafka_2.12-3.7.1/bin/zookeeper-shell.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-client-metrics.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-cluster.sh\n",
            "kafka_2.12-3.7.1/bin/windows/\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-e2e-latency.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-log-dirs.bat\n",
            "kafka_2.12-3.7.1/bin/windows/zookeeper-server-stop.bat\n",
            "kafka_2.12-3.7.1/bin/windows/connect-distributed.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-configs.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-console-producer.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-delete-records.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-topics.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-dump-log.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-console-consumer.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-server-start.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-consumer-groups.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-mirror-maker.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-features.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-reassign-partitions.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-client-metrics.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-cluster.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-producer-perf-test.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-jmx.bat\n",
            "kafka_2.12-3.7.1/bin/windows/zookeeper-server-start.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-server-stop.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-replica-verification.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-run-class.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-acls.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-delegation-tokens.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-broker-api-versions.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-metadata-quorum.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-transactions.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-storage.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-leader-election.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-streams-application-reset.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-get-offsets.bat\n",
            "kafka_2.12-3.7.1/bin/windows/connect-plugin-path.bat\n",
            "kafka_2.12-3.7.1/bin/windows/zookeeper-shell.bat\n",
            "kafka_2.12-3.7.1/bin/windows/connect-standalone.bat\n",
            "kafka_2.12-3.7.1/bin/windows/kafka-consumer-perf-test.bat\n",
            "kafka_2.12-3.7.1/bin/kafka-verifiable-producer.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-server-start.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-mirror-maker.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-storage.sh\n",
            "kafka_2.12-3.7.1/bin/kafka-streams-application-reset.sh\n",
            "kafka_2.12-3.7.1/bin/zookeeper-security-migration.sh\n",
            "kafka_2.12-3.7.1/config/\n",
            "kafka_2.12-3.7.1/config/consumer.properties\n",
            "kafka_2.12-3.7.1/config/connect-mirror-maker.properties\n",
            "kafka_2.12-3.7.1/config/zookeeper.properties\n",
            "kafka_2.12-3.7.1/config/server.properties\n",
            "kafka_2.12-3.7.1/config/producer.properties\n",
            "kafka_2.12-3.7.1/config/trogdor.conf\n",
            "kafka_2.12-3.7.1/config/connect-console-sink.properties\n",
            "kafka_2.12-3.7.1/config/connect-log4j.properties\n",
            "kafka_2.12-3.7.1/config/connect-standalone.properties\n",
            "kafka_2.12-3.7.1/config/connect-file-source.properties\n",
            "kafka_2.12-3.7.1/config/connect-console-source.properties\n",
            "kafka_2.12-3.7.1/config/connect-distributed.properties\n",
            "kafka_2.12-3.7.1/config/tools-log4j.properties\n",
            "kafka_2.12-3.7.1/config/connect-file-sink.properties\n",
            "kafka_2.12-3.7.1/config/kraft/\n",
            "kafka_2.12-3.7.1/config/kraft/server.properties\n",
            "kafka_2.12-3.7.1/config/kraft/broker.properties\n",
            "kafka_2.12-3.7.1/config/kraft/controller.properties\n",
            "kafka_2.12-3.7.1/config/log4j.properties\n",
            "kafka_2.12-3.7.1/licenses/\n",
            "kafka_2.12-3.7.1/licenses/argparse-MIT\n",
            "kafka_2.12-3.7.1/licenses/paranamer-BSD-3-clause\n",
            "kafka_2.12-3.7.1/licenses/zstd-jni-BSD-2-clause\n",
            "kafka_2.12-3.7.1/licenses/checker-qual-MIT\n",
            "kafka_2.12-3.7.1/licenses/CDDL+GPL-1.1\n",
            "kafka_2.12-3.7.1/licenses/jsr305-BSD-3-clause\n",
            "kafka_2.12-3.7.1/licenses/jline-BSD-3-clause\n",
            "kafka_2.12-3.7.1/licenses/eclipse-distribution-license-1.0\n",
            "kafka_2.12-3.7.1/licenses/slf4j-MIT\n",
            "kafka_2.12-3.7.1/licenses/jopt-simple-MIT\n",
            "kafka_2.12-3.7.1/licenses/protobuf-java-BSD-3-clause\n",
            "kafka_2.12-3.7.1/licenses/pcollections-MIT\n",
            "kafka_2.12-3.7.1/licenses/DWTFYWTPL\n",
            "kafka_2.12-3.7.1/licenses/eclipse-public-license-2.0\n",
            "kafka_2.12-3.7.1/libs/\n",
            "kafka_2.12-3.7.1/libs/kafka-group-coordinator-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/kafka-metadata-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/kafka-storage-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/kafka-storage-api-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/kafka-raft-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/kafka-server-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/kafka-server-common-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/kafka-tools-api-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/kafka-clients-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/jackson-dataformat-csv-2.16.2.jar\n",
            "kafka_2.12-3.7.1/libs/jackson-datatype-jdk8-2.16.2.jar\n",
            "kafka_2.12-3.7.1/libs/jackson-databind-2.16.2.jar\n",
            "kafka_2.12-3.7.1/libs/jackson-annotations-2.16.2.jar\n",
            "kafka_2.12-3.7.1/libs/jackson-core-2.16.2.jar\n",
            "kafka_2.12-3.7.1/libs/jackson-module-scala_2.12-2.16.2.jar\n",
            "kafka_2.12-3.7.1/libs/scala-collection-compat_2.12-2.10.0.jar\n",
            "kafka_2.12-3.7.1/libs/scala-java8-compat_2.12-1.0.2.jar\n",
            "kafka_2.12-3.7.1/libs/scala-logging_2.12-3.9.4.jar\n",
            "kafka_2.12-3.7.1/libs/scala-reflect-2.12.18.jar\n",
            "kafka_2.12-3.7.1/libs/scala-library-2.12.18.jar\n",
            "kafka_2.12-3.7.1/libs/argparse4j-0.7.0.jar\n",
            "kafka_2.12-3.7.1/libs/commons-validator-1.7.jar\n",
            "kafka_2.12-3.7.1/libs/jopt-simple-5.0.4.jar\n",
            "kafka_2.12-3.7.1/libs/jose4j-0.9.4.jar\n",
            "kafka_2.12-3.7.1/libs/metrics-core-2.2.0.jar\n",
            "kafka_2.12-3.7.1/libs/metrics-core-4.1.12.1.jar\n",
            "kafka_2.12-3.7.1/libs/zookeeper-3.8.4.jar\n",
            "kafka_2.12-3.7.1/libs/slf4j-api-1.7.36.jar\n",
            "kafka_2.12-3.7.1/libs/commons-cli-1.4.jar\n",
            "kafka_2.12-3.7.1/libs/commons-beanutils-1.9.4.jar\n",
            "kafka_2.12-3.7.1/libs/commons-logging-1.2.jar\n",
            "kafka_2.12-3.7.1/libs/commons-collections-3.2.2.jar\n",
            "kafka_2.12-3.7.1/libs/commons-digester-2.1.jar\n",
            "kafka_2.12-3.7.1/libs/paranamer-2.8.jar\n",
            "kafka_2.12-3.7.1/libs/zookeeper-jute-3.8.4.jar\n",
            "kafka_2.12-3.7.1/libs/audience-annotations-0.12.0.jar\n",
            "kafka_2.12-3.7.1/libs/netty-handler-4.1.110.Final.jar\n",
            "kafka_2.12-3.7.1/libs/netty-transport-native-epoll-4.1.110.Final.jar\n",
            "kafka_2.12-3.7.1/libs/netty-transport-classes-epoll-4.1.110.Final.jar\n",
            "kafka_2.12-3.7.1/libs/netty-transport-native-unix-common-4.1.110.Final.jar\n",
            "kafka_2.12-3.7.1/libs/netty-codec-4.1.110.Final.jar\n",
            "kafka_2.12-3.7.1/libs/netty-transport-4.1.110.Final.jar\n",
            "kafka_2.12-3.7.1/libs/netty-resolver-4.1.110.Final.jar\n",
            "kafka_2.12-3.7.1/libs/netty-buffer-4.1.110.Final.jar\n",
            "kafka_2.12-3.7.1/libs/netty-common-4.1.110.Final.jar\n",
            "kafka_2.12-3.7.1/libs/commons-io-2.11.0.jar\n",
            "kafka_2.12-3.7.1/libs/zstd-jni-1.5.6-3.jar\n",
            "kafka_2.12-3.7.1/libs/lz4-java-1.8.0.jar\n",
            "kafka_2.12-3.7.1/libs/snappy-java-1.1.10.5.jar\n",
            "kafka_2.12-3.7.1/libs/opentelemetry-proto-1.0.0-alpha.jar\n",
            "kafka_2.12-3.7.1/libs/pcollections-4.0.1.jar\n",
            "kafka_2.12-3.7.1/libs/caffeine-2.9.3.jar\n",
            "kafka_2.12-3.7.1/libs/protobuf-java-3.23.4.jar\n",
            "kafka_2.12-3.7.1/libs/checker-qual-3.19.0.jar\n",
            "kafka_2.12-3.7.1/libs/error_prone_annotations-2.10.0.jar\n",
            "kafka_2.12-3.7.1/libs/kafka_2.12-3.7.1.jar\n",
            "kafka_2.12-3.7.1/site-docs/\n",
            "kafka_2.12-3.7.1/site-docs/kafka_2.12-3.7.1-site-docs.tgz\n",
            "kafka_2.12-3.7.1/libs/kafka-tools-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/connect-runtime-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/kafka-log4j-appender-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/connect-json-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/connect-transforms-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/connect-api-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/reflections-0.10.2.jar\n",
            "kafka_2.12-3.7.1/libs/slf4j-reload4j-1.7.36.jar\n",
            "kafka_2.12-3.7.1/libs/reload4j-1.2.25.jar\n",
            "kafka_2.12-3.7.1/libs/jackson-jaxrs-base-2.16.2.jar\n",
            "kafka_2.12-3.7.1/libs/jackson-module-jaxb-annotations-2.16.2.jar\n",
            "kafka_2.12-3.7.1/libs/jackson-jaxrs-json-provider-2.16.2.jar\n",
            "kafka_2.12-3.7.1/libs/jakarta.xml.bind-api-2.3.3.jar\n",
            "kafka_2.12-3.7.1/libs/jakarta.activation-api-1.2.2.jar\n",
            "kafka_2.12-3.7.1/libs/jersey-container-servlet-2.39.1.jar\n",
            "kafka_2.12-3.7.1/libs/jersey-hk2-2.39.1.jar\n",
            "kafka_2.12-3.7.1/libs/jaxb-api-2.3.1.jar\n",
            "kafka_2.12-3.7.1/libs/activation-1.1.1.jar\n",
            "kafka_2.12-3.7.1/libs/jetty-servlet-9.4.54.v20240208.jar\n",
            "kafka_2.12-3.7.1/libs/jetty-security-9.4.54.v20240208.jar\n",
            "kafka_2.12-3.7.1/libs/jetty-server-9.4.54.v20240208.jar\n",
            "kafka_2.12-3.7.1/libs/jetty-servlets-9.4.54.v20240208.jar\n",
            "kafka_2.12-3.7.1/libs/jetty-client-9.4.54.v20240208.jar\n",
            "kafka_2.12-3.7.1/libs/maven-artifact-3.8.8.jar\n",
            "kafka_2.12-3.7.1/libs/swagger-annotations-2.2.8.jar\n",
            "kafka_2.12-3.7.1/libs/javax.ws.rs-api-2.1.1.jar\n",
            "kafka_2.12-3.7.1/libs/jersey-container-servlet-core-2.39.1.jar\n",
            "kafka_2.12-3.7.1/libs/jersey-server-2.39.1.jar\n",
            "kafka_2.12-3.7.1/libs/jersey-client-2.39.1.jar\n",
            "kafka_2.12-3.7.1/libs/jersey-common-2.39.1.jar\n",
            "kafka_2.12-3.7.1/libs/jakarta.ws.rs-api-2.1.6.jar\n",
            "kafka_2.12-3.7.1/libs/hk2-locator-2.6.1.jar\n",
            "kafka_2.12-3.7.1/libs/javassist-3.29.2-GA.jar\n",
            "kafka_2.12-3.7.1/libs/javax.activation-api-1.2.0.jar\n",
            "kafka_2.12-3.7.1/libs/javax.servlet-api-3.1.0.jar\n",
            "kafka_2.12-3.7.1/libs/jetty-http-9.4.54.v20240208.jar\n",
            "kafka_2.12-3.7.1/libs/jetty-io-9.4.54.v20240208.jar\n",
            "kafka_2.12-3.7.1/libs/jetty-util-ajax-9.4.54.v20240208.jar\n",
            "kafka_2.12-3.7.1/libs/jetty-continuation-9.4.54.v20240208.jar\n",
            "kafka_2.12-3.7.1/libs/jetty-util-9.4.54.v20240208.jar\n",
            "kafka_2.12-3.7.1/libs/jsr305-3.0.2.jar\n",
            "kafka_2.12-3.7.1/libs/plexus-utils-3.3.1.jar\n",
            "kafka_2.12-3.7.1/libs/commons-lang3-3.8.1.jar\n",
            "kafka_2.12-3.7.1/libs/hk2-api-2.6.1.jar\n",
            "kafka_2.12-3.7.1/libs/hk2-utils-2.6.1.jar\n",
            "kafka_2.12-3.7.1/libs/jakarta.inject-2.6.1.jar\n",
            "kafka_2.12-3.7.1/libs/jakarta.annotation-api-1.3.5.jar\n",
            "kafka_2.12-3.7.1/libs/osgi-resource-locator-1.0.3.jar\n",
            "kafka_2.12-3.7.1/libs/jakarta.validation-api-2.0.2.jar\n",
            "kafka_2.12-3.7.1/libs/aopalliance-repackaged-2.6.1.jar\n",
            "kafka_2.12-3.7.1/libs/trogdor-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/kafka-shell-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/jline-3.25.1.jar\n",
            "kafka_2.12-3.7.1/libs/connect-file-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/connect-basic-auth-extension-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/javax.annotation-api-1.3.2.jar\n",
            "kafka_2.12-3.7.1/libs/connect-mirror-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/connect-mirror-client-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/kafka-streams-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/rocksdbjni-7.9.2.jar\n",
            "kafka_2.12-3.7.1/libs/kafka-streams-scala_2.12-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/kafka-streams-test-utils-3.7.1.jar\n",
            "kafka_2.12-3.7.1/libs/kafka-streams-examples-3.7.1.jar\n"
          ]
        }
      ],
      "source": [
        "!curl -sSOL https://downloads.apache.org/kafka/3.7.1/kafka_2.12-3.7.1.tgz\n",
        "!tar -xvzf kafka_2.12-3.7.1.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAzfu_WiEs4F"
      },
      "source": [
        "Kafka with defaults"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltrh"
      ],
      "metadata": {
        "id": "JEYWVgkrn50h",
        "outputId": "ee8377a0-7adf-4843-a8ef-3b65b70ed2f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 115M\n",
            "drwxr-xr-x 7 root root 4.0K Jun 18 21:32 kafka_2.12-3.7.1\n",
            "drwxr-xr-x 1 root root 4.0K Oct 14 13:23 sample_data\n",
            "-rw-r--r-- 1 root root 1.1K Oct 17 22:32 generator.py\n",
            "-rw-r--r-- 1 root root  196 Oct 17 22:35 kafka_2.12-3.7.0.tgz\n",
            "-rw-r--r-- 1 root root 115M Oct 17 22:38 kafka_2.12-3.7.1.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "n9ujlunrWgRx",
        "outputId": "bec1d4fb-db11-4968-de87-d267705ed1b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the processes 10 seconds to start before proceeding.\n"
          ]
        }
      ],
      "source": [
        "!./kafka_2.12-3.7.1/bin/zookeeper-server-start.sh -daemon ./kafka_2.12-3.7.1/config/zookeeper.properties\n",
        "!./kafka_2.12-3.7.1/bin/kafka-server-start.sh -daemon ./kafka_2.12-3.7.1/config/server.properties\n",
        "!echo \"Give the processes 10 seconds to start before proceeding.\"\n",
        "!sleep 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6qxCdypE1DD"
      },
      "source": [
        "Is Kafka running?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "48LqMJ1BEHm5",
        "outputId": "34988fc7-36c2-424a-e413-0bc6cd7bfe3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        2965       1 22 22:38 ?        00:00:03 java -Xmx512M -Xms512M -server -XX:+UseG1GC -XX:\n",
            "root        3391       1 58 22:38 ?        00:00:06 java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxG\n",
            "root        3483     826  0 22:38 ?        00:00:00 /bin/bash -c ps -ef | grep java\n",
            "root        3485    3483  0 22:38 ?        00:00:00 grep java\n"
          ]
        }
      ],
      "source": [
        "!ps -ef | grep java"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3TntBqanQnh"
      },
      "source": [
        "Create the kafka topics with the following specs:\n",
        "\n",
        "- sample-streaming-data: partitions=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lXJWqMmWnPyP",
        "outputId": "bb47eeda-de7a-4d25-a7d4-e354fdfa6dde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created topic sample-streaming-data.\n"
          ]
        }
      ],
      "source": [
        "!./kafka_2.12-3.7.1/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic sample-streaming-data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNxf_NqjnycC"
      },
      "source": [
        "Describe the topic for details on the configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "apCf9pfVnwn7",
        "outputId": "0b4fd307-5d12-4cb3-8c82-8d3cab78702a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: sample-streaming-data\tTopicId: zQ34WgYlTxe6VL58VDnmYA\tPartitionCount: 1\tReplicationFactor: 1\tConfigs: \n",
            "\tTopic: sample-streaming-data\tPartition: 0\tLeader: 0\tReplicas: 0\tIsr: 0\n"
          ]
        }
      ],
      "source": [
        "!./kafka_2.12-3.7.1/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic sample-streaming-data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generator python script\n",
        "\n",
        "This script simply generates random data to publish into our topic"
      ],
      "metadata": {
        "id": "eD6MgELGdYrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile generator.py\n",
        "\n",
        "import sys\n",
        "args = sys.argv  # a list of the arguments provided (str)\n",
        "print(\"running generator.py\", args)\n",
        "iterations = int(args[1])\n",
        "print(f'iterations: {iterations}')\n",
        "\n",
        "def error_callback(exc):\n",
        "    raise Exception('Error while sendig data to kafka: {0}'.format(str(exc)))\n",
        "\n",
        "def write_to_kafka(topic_name, items):\n",
        "  from kafka import KafkaProducer\n",
        "\n",
        "  count=0\n",
        "  producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])\n",
        "  for message, key in items:\n",
        "    producer.send(topic_name, key=key.encode('utf-8'), value=message.encode('utf-8'), partition=0).add_errback(error_callback)\n",
        "    count+=1\n",
        "  producer.flush()\n",
        "  print(\"Wrote {0} messages into topic: {1}\".format(count, topic_name))\n",
        "\n",
        "import random\n",
        "from time import sleep\n",
        "\n",
        "def generate_data(rows=2):\n",
        "\n",
        "  index_num = random.randint(0,1000000)\n",
        "  print(index_num)\n",
        "  keys = list([f'{index_num}'])\n",
        "  msg = list([f'hello world!{index_num}'])\n",
        "  data = zip(msg, keys)\n",
        "\n",
        "  return data\n",
        "\n",
        "for i in range(iterations):\n",
        "  write_to_kafka(\"sample-streaming-data\", generate_data())\n",
        "  sleep(random.randint(0,5))\n",
        "\n"
      ],
      "metadata": {
        "id": "jXTRHjchRzQP",
        "outputId": "b8b09848-4744-4d29-a8a8-ac1d036cc9ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting generator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# write some data"
      ],
      "metadata": {
        "id": "xslSU8wFe9uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script bash --bg\n",
        "\n",
        "python generator.py 10"
      ],
      "metadata": {
        "id": "8KuaSYrRSMTu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message_n = 10\n",
        "\n",
        "from kafka import KafkaConsumer\n",
        "\n",
        "# Kafka consumer configuration\n",
        "bootstrap_servers = ['localhost:9092']  # Kafka server address\n",
        "topic_name = 'sample-streaming-data'  # Kafka topic you want to read from\n",
        "group_id = 'some_group'  # Consumer group ID\n",
        "\n",
        "# Create a Kafka consumer\n",
        "consumer = KafkaConsumer(\n",
        "    topic_name,\n",
        "    bootstrap_servers=bootstrap_servers,\n",
        "    auto_offset_reset='earliest',  # Start reading at the earliest message\n",
        "    enable_auto_commit=True,\n",
        "    group_id=group_id,\n",
        "    value_deserializer=lambda x: x.decode('utf-8')  # Assuming messages are UTF-8 encoded\n",
        ")\n",
        "\n",
        "# Read and print messages from the topic\n",
        "try:\n",
        "    for _ in range(message_n):\n",
        "        message = next(consumer)\n",
        "        print(f\"Received message: {message.value}\")\n",
        "finally:\n",
        "    # Clean up on exit\n",
        "    consumer.close()\n"
      ],
      "metadata": {
        "id": "T39K8fkp3b6k",
        "outputId": "edb31452-c88c-4ca4-ae0d-f4db3cbd1770",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Received message: hello world!758992\n",
            "Received message: hello world!210415\n",
            "Received message: hello world!229729\n",
            "Received message: hello world!788082\n",
            "Received message: hello world!890585\n",
            "Received message: hello world!148108\n",
            "Received message: hello world!955064\n",
            "Received message: hello world!430529\n",
            "Received message: hello world!91747\n",
            "Received message: hello world!171679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTA is no longer working with SIRI service"
      ],
      "metadata": {
        "id": "kbLaxfHwr299"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTA API\n",
        "\n",
        "\n",
        "We'll retrieve realtime data from UTA and publish it to our kafka topic as a producter.\n",
        "\n",
        "Then we'll interact with the topic as a Consumer and get the messages out of the topic"
      ],
      "metadata": {
        "id": "mzeaaH5p2Orz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install package needed for uta api"
      ],
      "metadata": {
        "id": "Y2XE5DG-1Gyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xmltodict"
      ],
      "metadata": {
        "id": "bKAwgKQB3RQB",
        "outputId": "6c21ac7a-fcfe-41b1-aec6-8b577ecaafbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Downloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: xmltodict\n",
            "Successfully installed xmltodict-0.14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create a dict of our token to save to a file for the python script"
      ],
      "metadata": {
        "id": "1XP0PBvo1Ash"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_dict = {'token':userdata.get('uta')}"
      ],
      "metadata": {
        "id": "h051grd7AOkH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dump the token data to a file"
      ],
      "metadata": {
        "id": "5fLsJHdK1Ksv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('token.json', 'w') as file:\n",
        "    json.dump(token_dict, file)"
      ],
      "metadata": {
        "id": "4sN3sIt3C49O"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = userdata.get('uta')"
      ],
      "metadata": {
        "id": "leXJo95-pct5"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## retrieve the data from the API and write it to the topic.\n",
        "\n",
        "these scripts will retrieve data from the API and write the messages to the kafka topic"
      ],
      "metadata": {
        "id": "HTsKpiAR2rz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile uta_generator.py\n",
        "\n",
        "from google.colab import userdata\n",
        "import json\n",
        "import xmltodict\n",
        "import sys\n",
        "\n",
        "\n",
        "args = sys.argv  # a list of the arguments provided (str)\n",
        "print(\"running generator.py\", args)\n",
        "iterations = int(args[1])\n",
        "\n",
        "print(f'iterations: {iterations}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def error_callback(exc):\n",
        "    raise Exception('Error while sendig data to kafka: {0}'.format(str(exc)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# take the data retrieved from the api and write it to the kafka topic\n",
        "def write_to_kafka(topic_name, items):\n",
        "  from kafka import KafkaProducer\n",
        "\n",
        "  count=0\n",
        "  producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])\n",
        "  print(items)\n",
        "\n",
        "  for ref, lat, lon in items:\n",
        "    print(ref, lat, lon)\n",
        "    location = f'{{\"Vehichle_ref\":{ref},Latitude\": {lat}, \"Longitude\": {lon}}}'\n",
        "    producer.send(topic_name, value=location.encode('utf-8'), partition=0).add_errback(error_callback)\n",
        "    count+=1\n",
        "\n",
        "  producer.flush()\n",
        "  print(\"Wrote {0} messages into topic: {1}\".format(count, topic_name))\n",
        "\n",
        "\n",
        "# the api call to the uta api\n",
        "def get_locations(token):\n",
        "\n",
        "    from time import sleep\n",
        "    from google.colab import userdata\n",
        "    import requests\n",
        "    import xmltodict\n",
        "    import pandas as pd\n",
        "    import os\n",
        "\n",
        "    url = f'http://api.rideuta.com/SIRI/SIRI.svc/VehicleMonitor/ByRoute?route=703&onwardcalls=true&usertoken={token}'\n",
        "    print(url)\n",
        "    response = requests.get(url)\n",
        "    xml_dict = xmltodict.parse(response.text)\n",
        "    df = pd.DataFrame(xml_dict['Siri']['VehicleMonitoringDelivery']['VehicleActivity']['MonitoredVehicleJourney'])\n",
        "    print(df)\n",
        "    print(df.VehicleRef.value_counts())\n",
        "    print(df.shape)\n",
        "\n",
        "    location_df = pd.json_normalize(df.VehicleLocation)\n",
        "    vehicle_location = pd.merge(df['VehicleRef'],location_df,left_index=True,right_index=True)\n",
        "    zip_tuple = tuple(zip(vehicle_location['VehicleRef'],vehicle_location['Latitude'], vehicle_location['Longitude']))\n",
        "    sleep(6)\n",
        "\n",
        "    return zip_tuple\n",
        "\n",
        "# open the token file\n",
        "with open('token.json', 'r') as file:\n",
        "    token_dict = json.load(file)\n",
        "\n",
        "# turn the dict value into a local var\n",
        "token = token_dict['token']\n",
        "\n",
        "# call the function multiple times to get the data\n",
        "# and write it to the kafka topic\n",
        "\n",
        "for i in range(iterations):\n",
        "  write_to_kafka(\"sample-streaming-data\", get_locations(token))\n",
        "\n"
      ],
      "metadata": {
        "id": "unUISCZE2528",
        "outputId": "93232772-6c15-428a-b6bc-f3b4fe1f46f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing uta_generator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## let's remind ourselves of the UTA data structure\n",
        "\n",
        "create a function to hit the api using our token"
      ],
      "metadata": {
        "id": "xRT6egH2-FgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_red_trains(token):\n",
        "    from time import sleep\n",
        "    from google.colab import userdata\n",
        "    import requests\n",
        "    import xmltodict\n",
        "    import pandas as pd\n",
        "\n",
        "    token = userdata.get('uta')\n",
        "    url = f'http://api.rideuta.com/SIRI/SIRI.svc/VehicleMonitor/ByRoute?route=703&onwardcalls=true&usertoken={token}'\n",
        "    response = requests.get(url)\n",
        "    xml_dict = xmltodict.parse(response.text)\n",
        "    df = pd.DataFrame(xml_dict['Siri']['VehicleMonitoringDelivery']['VehicleActivity']['MonitoredVehicleJourney'])\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "K3q2gT5k4aLM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "7r0aolTOpkLp"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LcDkui9Ap3vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    url = f'http://api.rideuta.com/SIRI/SIRI.svc/VehicleMonitor/ByRoute?route=703&onwardcalls=true&usertoken={token}'\n",
        "    requests.get(url)"
      ],
      "metadata": {
        "id": "PTT3FB8EpDK7",
        "outputId": "41cf6842-b648-44d7-9cfb-814b48515172",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [503]>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hit the api and show the dataframe we have from the request."
      ],
      "metadata": {
        "id": "18snbmZ4-7BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = get_red_trains(userdata.get('uta'))\n",
        "df"
      ],
      "metadata": {
        "id": "yVsmn9eM4dw_",
        "outputId": "dabc2955-8c6e-4119-c219-da5df8bae33a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ExpatError",
          "evalue": "not well-formed (invalid token): line 1, column 49",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mExpatError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-f73891bfb35f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_red_trains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-2e5819bf5271>\u001b[0m in \u001b[0;36mget_red_trains\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'http://api.rideuta.com/SIRI/SIRI.svc/VehicleMonitor/ByRoute?route=703&onwardcalls=true&usertoken={token}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mxml_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxmltodict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Siri'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'VehicleMonitoringDelivery'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'VehicleActivity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MonitoredVehicleJourney'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xmltodict.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(xml_input, encoding, expat, process_namespaces, namespace_separator, disable_entities, process_comments, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mExpatError\u001b[0m: not well-formed (invalid token): line 1, column 49"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get train lat/lon\n",
        "\n",
        "conver the JSON data to a dataframe format after normalizing the lat/lon data"
      ],
      "metadata": {
        "id": "8A-2iHBw_CP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "location_df = pd.json_normalize(df.VehicleLocation)\n",
        "vehicle_location = pd.merge(df['VehicleRef'],location_df,left_index=True,right_index=True)\n",
        "vehicle_location"
      ],
      "metadata": {
        "id": "sMECFfb7gXz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get UTA data\n",
        "\n",
        "this calls the script asyncronously so our cells are not blocked from execution while the API data is retrieved"
      ],
      "metadata": {
        "id": "-ED3TksR_PZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script bash --bg\n",
        "\n",
        "python uta_generator.py 10 >> log.txt"
      ],
      "metadata": {
        "id": "78C9S_ft7Dgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KAFKA Consumer for UTA"
      ],
      "metadata": {
        "id": "XltZo2_P_fOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message_n = 100\n",
        "\n",
        "messages = []\n",
        "\n",
        "from kafka import KafkaConsumer\n",
        "\n",
        "# Kafka consumer configuration\n",
        "bootstrap_servers = ['localhost:9092']  # Kafka server address\n",
        "topic_name = 'sample-streaming-data'  # Kafka topic you want to read from\n",
        "group_id = 'some_group'  # Consumer group ID\n",
        "\n",
        "# Create a Kafka consumer\n",
        "consumer = KafkaConsumer(\n",
        "    topic_name,\n",
        "    bootstrap_servers=bootstrap_servers,\n",
        "    auto_offset_reset='earliest',  # Start reading at the earliest message\n",
        "    enable_auto_commit=True,\n",
        "    group_id=group_id,\n",
        "    value_deserializer=lambda x: x.decode('utf-8')  # Assuming messages are UTF-8 encoded\n",
        ")\n",
        "\n",
        "# Read and print five messages from the topic\n",
        "try:\n",
        "    for _ in range(message_n):\n",
        "        message = next(consumer)\n",
        "        messages.append(message)\n",
        "        print(f\"Received message: {message.value}\")\n",
        "finally:\n",
        "    # Clean up on exit\n",
        "    consumer.close()\n"
      ],
      "metadata": {
        "id": "WPWaAF0vz4Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function for retrieve messages."
      ],
      "metadata": {
        "id": "cc6L3ezS5eny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message"
      ],
      "metadata": {
        "id": "ECNRwsjG2JQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# retrieve messages"
      ],
      "metadata": {
        "id": "qNigtAVzv89C"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}