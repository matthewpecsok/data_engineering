{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewpecsok/data_engineering/blob/main/tutorials/de_streaming_kafka_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upgCc3gXybsA"
      },
      "source": [
        "### Install the required kafka packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48B9eAMMhAgw"
      },
      "outputs": [],
      "source": [
        "!pip install kafka-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjrZNJQRJP-U"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6KXZuTBWgRm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from datetime import datetime\n",
        "import time\n",
        "import threading\n",
        "import json\n",
        "from kafka import KafkaProducer\n",
        "from kafka.errors import KafkaError\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZmI7l_GykcW"
      },
      "source": [
        "## Download and setup Kafka and Zookeeper instances\n",
        "\n",
        "For demo purposes, the following instances are setup locally:\n",
        "\n",
        "- Kafka (Brokers: 127.0.0.1:9092)\n",
        "- Zookeeper (Node: 127.0.0.1:2181)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUj0878jPyz7"
      },
      "outputs": [],
      "source": [
        "!curl -sSOL https://downloads.apache.org/kafka/3.7.0/kafka_2.12-3.7.0.tgz\n",
        "!tar -xzf kafka_2.12-3.7.0.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAzfu_WiEs4F"
      },
      "source": [
        "Kafka with defaults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9ujlunrWgRx"
      },
      "outputs": [],
      "source": [
        "!./kafka_2.12-3.7.0/bin/zookeeper-server-start.sh -daemon ./kafka_2.12-3.7.0/config/zookeeper.properties\n",
        "!./kafka_2.12-3.7.0/bin/kafka-server-start.sh -daemon ./kafka_2.12-3.7.0/config/server.properties\n",
        "!echo \"Give the processes 10 seconds to start before proceeding.\"\n",
        "!sleep 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6qxCdypE1DD"
      },
      "source": [
        "Is Kafka running?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48LqMJ1BEHm5"
      },
      "outputs": [],
      "source": [
        "!ps -ef | grep java"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3TntBqanQnh"
      },
      "source": [
        "Create the kafka topics with the following specs:\n",
        "\n",
        "- sample-streaming-data: partitions=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXJWqMmWnPyP"
      },
      "outputs": [],
      "source": [
        "!./kafka_2.12-3.7.0/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic sample-streaming-data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNxf_NqjnycC"
      },
      "source": [
        "Describe the topic for details on the configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apCf9pfVnwn7"
      },
      "outputs": [],
      "source": [
        "!./kafka_2.12-3.7.0/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic sample-streaming-data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generator python script\n",
        "\n",
        "This script simply generates random data to publish into our topic"
      ],
      "metadata": {
        "id": "eD6MgELGdYrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile generator.py\n",
        "\n",
        "import sys\n",
        "args = sys.argv  # a list of the arguments provided (str)\n",
        "print(\"running generator.py\", args)\n",
        "iterations = int(args[1])\n",
        "print(f'iterations: {iterations}')\n",
        "\n",
        "def error_callback(exc):\n",
        "    raise Exception('Error while sendig data to kafka: {0}'.format(str(exc)))\n",
        "\n",
        "def write_to_kafka(topic_name, items):\n",
        "  from kafka import KafkaProducer\n",
        "\n",
        "  count=0\n",
        "  producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])\n",
        "  for message, key in items:\n",
        "    producer.send(topic_name, key=key.encode('utf-8'), value=message.encode('utf-8'), partition=0).add_errback(error_callback)\n",
        "    count+=1\n",
        "  producer.flush()\n",
        "  print(\"Wrote {0} messages into topic: {1}\".format(count, topic_name))\n",
        "\n",
        "import random\n",
        "from time import sleep\n",
        "\n",
        "def generate_data(rows=2):\n",
        "\n",
        "  index_num = random.randint(0,1000000)\n",
        "  print(index_num)\n",
        "  keys = list([f'{index_num}'])\n",
        "  msg = list([f'hello world!{index_num}'])\n",
        "  data = zip(msg, keys)\n",
        "\n",
        "  return data\n",
        "\n",
        "for i in range(iterations):\n",
        "  write_to_kafka(\"sample-streaming-data\", generate_data())\n",
        "  sleep(random.randint(0,5))\n",
        "\n"
      ],
      "metadata": {
        "id": "jXTRHjchRzQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# write some data"
      ],
      "metadata": {
        "id": "xslSU8wFe9uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script bash --bg\n",
        "\n",
        "python generator.py 10"
      ],
      "metadata": {
        "id": "8KuaSYrRSMTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message_n = 10\n",
        "\n",
        "from kafka import KafkaConsumer\n",
        "\n",
        "# Kafka consumer configuration\n",
        "bootstrap_servers = ['localhost:9092']  # Kafka server address\n",
        "topic_name = 'sample-streaming-data'  # Kafka topic you want to read from\n",
        "group_id = 'some_group'  # Consumer group ID\n",
        "\n",
        "# Create a Kafka consumer\n",
        "consumer = KafkaConsumer(\n",
        "    topic_name,\n",
        "    bootstrap_servers=bootstrap_servers,\n",
        "    auto_offset_reset='earliest',  # Start reading at the earliest message\n",
        "    enable_auto_commit=True,\n",
        "    group_id=group_id,\n",
        "    value_deserializer=lambda x: x.decode('utf-8')  # Assuming messages are UTF-8 encoded\n",
        ")\n",
        "\n",
        "# Read and print messages from the topic\n",
        "try:\n",
        "    for _ in range(message_n):\n",
        "        message = next(consumer)\n",
        "        print(f\"Received message: {message.value}\")\n",
        "finally:\n",
        "    # Clean up on exit\n",
        "    consumer.close()\n"
      ],
      "metadata": {
        "id": "T39K8fkp3b6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTA API\n",
        "\n",
        "We'll retrieve realtime data from UTA and publish it to our kafka topic as a producter.\n",
        "\n",
        "Then we'll interact with the topic as a Consumer and get the messages out of the topic"
      ],
      "metadata": {
        "id": "mzeaaH5p2Orz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install package needed for uta api"
      ],
      "metadata": {
        "id": "Y2XE5DG-1Gyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xmltodict"
      ],
      "metadata": {
        "id": "bKAwgKQB3RQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create a dict of our token to save to a file for the python script"
      ],
      "metadata": {
        "id": "1XP0PBvo1Ash"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_dict = {'token':userdata.get('uta')}"
      ],
      "metadata": {
        "id": "h051grd7AOkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dump the token data to a file"
      ],
      "metadata": {
        "id": "5fLsJHdK1Ksv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('token.json', 'w') as file:\n",
        "    json.dump(token_dict, file)"
      ],
      "metadata": {
        "id": "4sN3sIt3C49O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## retrieve the data from the API and write it to the topic.\n",
        "\n",
        "these scripts will retrieve data from the API and write the messages to the kafka topic"
      ],
      "metadata": {
        "id": "HTsKpiAR2rz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile uta_generator.py\n",
        "\n",
        "from google.colab import userdata\n",
        "import json\n",
        "import xmltodict\n",
        "import sys\n",
        "\n",
        "\n",
        "args = sys.argv  # a list of the arguments provided (str)\n",
        "print(\"running generator.py\", args)\n",
        "iterations = int(args[1])\n",
        "\n",
        "print(f'iterations: {iterations}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def error_callback(exc):\n",
        "    raise Exception('Error while sendig data to kafka: {0}'.format(str(exc)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# take the data retrieved from the api and write it to the kafka topic\n",
        "def write_to_kafka(topic_name, items):\n",
        "  from kafka import KafkaProducer\n",
        "\n",
        "  count=0\n",
        "  producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])\n",
        "  print(items)\n",
        "\n",
        "  for ref, lat, lon in items:\n",
        "    print(ref, lat, lon)\n",
        "    location = f'{{\"Vehichle_ref\":{ref},Latitude\": {lat}, \"Longitude\": {lon}}}'\n",
        "    producer.send(topic_name, value=location.encode('utf-8'), partition=0).add_errback(error_callback)\n",
        "    count+=1\n",
        "\n",
        "  producer.flush()\n",
        "  print(\"Wrote {0} messages into topic: {1}\".format(count, topic_name))\n",
        "\n",
        "\n",
        "# the api call to the uta api\n",
        "def get_locations(token):\n",
        "\n",
        "    from time import sleep\n",
        "    from google.colab import userdata\n",
        "    import requests\n",
        "    import xmltodict\n",
        "    import pandas as pd\n",
        "    import os\n",
        "\n",
        "    url = f'http://api.rideuta.com/SIRI/SIRI.svc/VehicleMonitor/ByRoute?route=703&onwardcalls=true&usertoken={token}'\n",
        "    print(url)\n",
        "    response = requests.get(url)\n",
        "    xml_dict = xmltodict.parse(response.text)\n",
        "    df = pd.DataFrame(xml_dict['Siri']['VehicleMonitoringDelivery']['VehicleActivity']['MonitoredVehicleJourney'])\n",
        "    print(df)\n",
        "    print(df.VehicleRef.value_counts())\n",
        "    print(df.shape)\n",
        "\n",
        "    location_df = pd.json_normalize(df.VehicleLocation)\n",
        "    vehicle_location = pd.merge(df['VehicleRef'],location_df,left_index=True,right_index=True)\n",
        "    zip_tuple = tuple(zip(vehicle_location['VehicleRef'],vehicle_location['Latitude'], vehicle_location['Longitude']))\n",
        "    sleep(6)\n",
        "\n",
        "    return zip_tuple\n",
        "\n",
        "# open the token file\n",
        "with open('token.json', 'r') as file:\n",
        "    token_dict = json.load(file)\n",
        "\n",
        "# turn the dict value into a local var\n",
        "token = token_dict['token']\n",
        "\n",
        "# call the function multiple times to get the data\n",
        "# and write it to the kafka topic\n",
        "\n",
        "for i in range(iterations):\n",
        "  write_to_kafka(\"sample-streaming-data\", get_locations(token))\n",
        "\n"
      ],
      "metadata": {
        "id": "unUISCZE2528"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## let's remind ourselves of the UTA data structure\n",
        "\n",
        "create a function to hit the api using our token"
      ],
      "metadata": {
        "id": "xRT6egH2-FgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_red_trains(token):\n",
        "    from time import sleep\n",
        "    from google.colab import userdata\n",
        "    import requests\n",
        "    import xmltodict\n",
        "    import pandas as pd\n",
        "\n",
        "    token = userdata.get('uta')\n",
        "    url = f'http://api.rideuta.com/SIRI/SIRI.svc/VehicleMonitor/ByRoute?route=703&onwardcalls=true&usertoken={token}'\n",
        "    response = requests.get(url)\n",
        "    xml_dict = xmltodict.parse(response.text)\n",
        "    df = pd.DataFrame(xml_dict['Siri']['VehicleMonitoringDelivery']['VehicleActivity']['MonitoredVehicleJourney'])\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "K3q2gT5k4aLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "hit the api and show the dataframe we have from the request."
      ],
      "metadata": {
        "id": "18snbmZ4-7BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = get_red_trains(userdata.get('uta'))\n",
        "df"
      ],
      "metadata": {
        "id": "yVsmn9eM4dw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get train lat/lon\n",
        "\n",
        "conver the JSON data to a dataframe format after normalizing the lat/lon data"
      ],
      "metadata": {
        "id": "8A-2iHBw_CP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "location_df = pd.json_normalize(df.VehicleLocation)\n",
        "vehicle_location = pd.merge(df['VehicleRef'],location_df,left_index=True,right_index=True)\n",
        "vehicle_location"
      ],
      "metadata": {
        "id": "sMECFfb7gXz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get UTA data\n",
        "\n",
        "this calls the script asyncronously so our cells are not blocked from execution while the API data is retrieved"
      ],
      "metadata": {
        "id": "-ED3TksR_PZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script bash --bg\n",
        "\n",
        "python uta_generator.py 10 >> log.txt"
      ],
      "metadata": {
        "id": "78C9S_ft7Dgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KAFKA Consumer for UTA"
      ],
      "metadata": {
        "id": "XltZo2_P_fOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message_n = 100\n",
        "\n",
        "messages = []\n",
        "\n",
        "from kafka import KafkaConsumer\n",
        "\n",
        "# Kafka consumer configuration\n",
        "bootstrap_servers = ['localhost:9092']  # Kafka server address\n",
        "topic_name = 'sample-streaming-data'  # Kafka topic you want to read from\n",
        "group_id = 'some_group'  # Consumer group ID\n",
        "\n",
        "# Create a Kafka consumer\n",
        "consumer = KafkaConsumer(\n",
        "    topic_name,\n",
        "    bootstrap_servers=bootstrap_servers,\n",
        "    auto_offset_reset='earliest',  # Start reading at the earliest message\n",
        "    enable_auto_commit=True,\n",
        "    group_id=group_id,\n",
        "    value_deserializer=lambda x: x.decode('utf-8')  # Assuming messages are UTF-8 encoded\n",
        ")\n",
        "\n",
        "# Read and print five messages from the topic\n",
        "try:\n",
        "    for _ in range(message_n):\n",
        "        message = next(consumer)\n",
        "        messages.append(message)\n",
        "        print(f\"Received message: {message.value}\")\n",
        "finally:\n",
        "    # Clean up on exit\n",
        "    consumer.close()\n"
      ],
      "metadata": {
        "id": "WPWaAF0vz4Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function for retrieve messages."
      ],
      "metadata": {
        "id": "cc6L3ezS5eny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message"
      ],
      "metadata": {
        "id": "ECNRwsjG2JQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# retrieve messages"
      ],
      "metadata": {
        "id": "qNigtAVzv89C"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}