{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPA4iZQXu2LAXmdhmQC22Bn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewpecsok/data_engineering/blob/main/tutorials/de_object_storage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this tutorial requires you to have created a GCP project and a storage bucket created.\n",
        "\n",
        "1.    Log into console.cloud.google.com\n",
        "2.    Create a storage bucket\n",
        "3.    Create an IAM service account with Storage Object User Role\n",
        "4.    Add a KEY (JSON)\n",
        "5.    Upload the key to you colab VM"
      ],
      "metadata": {
        "id": "ddcKatvQlu6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "import sqlite3\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "SIbs3UNBmPd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/fleet-space-407416-a177ff0a8af7.json\""
      ],
      "metadata": {
        "id": "5QQDs7rSm-FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "download the northwind sqlite db"
      ],
      "metadata": {
        "id": "G44O7ZzkjzC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O northwind.db https://github.com/matthewpecsok/data_engineering/raw/main/data/northwind.db"
      ],
      "metadata": {
        "id": "vASKMnXHoIYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a connection object"
      ],
      "metadata": {
        "id": "EI8SUxTyj13i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect(\"northwind.db\")"
      ],
      "metadata": {
        "id": "BDI8yLBGR2Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "using pandas and the connection object retrieve a list of table names from the database"
      ],
      "metadata": {
        "id": "IwylpQ5jj3nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)"
      ],
      "metadata": {
        "id": "1dGF9aUVWWBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   read the customers table into a pandas dataframe\n",
        "2.   write the dataframe to a csv file (exclude the index)\n",
        "3.   using the shell command tail show the last 10 rows of the csv file.\n",
        "\n"
      ],
      "metadata": {
        "id": "oI5ANSCRj-_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers = pd.read_sql(\"SELECT * FROM customers\", conn)\n",
        "customers.to_csv(\"customers.csv\",index=False)\n",
        "!tail customers.csv"
      ],
      "metadata": {
        "id": "8f0CCGveR-bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   read the orders table into a pandas dataframe\n",
        "2.   write the dataframe to a csv file (exclude the index)\n",
        "3.   using the shell command tail show the last 10 rows of the csv file.\n"
      ],
      "metadata": {
        "id": "u9cOo4vLkVT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders = pd.read_sql(\"SELECT * FROM orders\", conn)\n",
        "orders.to_csv(\"orders.csv\",index=False)\n",
        "!tail orders.csv"
      ],
      "metadata": {
        "id": "JpSspcN2S_Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   read the order detail table into a pandas dataframe\n",
        "2.   write the dataframe to a csv file (exclude the index)\n",
        "3.   using the shell command tail show the last 10 rows of the csv file.\n"
      ],
      "metadata": {
        "id": "7BKrD1bmkXZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "order_detail = pd.read_sql(\"SELECT * FROM 'Order Details'\", conn)\n",
        "order_detail.to_csv(\"order_detail.csv\",index=False)\n",
        "!tail order_detail.csv"
      ],
      "metadata": {
        "id": "VXjkSgsgWAfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve6_pQ9clTGd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# create a new function for uploading to GCP cloud storage\n",
        "def upload_to_storage(bucket_name, source_file_name, destination_blob_name):\n",
        "    from google.cloud import storage\n",
        "\n",
        "    storage_client = storage.Client() # create the Client.\n",
        "    bucket = storage_client.bucket(bucket_name) # get the bucket instance\n",
        "    blob = bucket.blob(destination_blob_name) # create a new blob\n",
        "\n",
        "    blob.upload_from_filename(source_file_name) # upload the file\n",
        "\n",
        "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\") # print the success message\n",
        "\n",
        "# create a new function for downloading from GCP cloud storage\n",
        "def download_from_storage(bucket_name, source_file_name, destination_blob_name):\n",
        "    from google.cloud import storage\n",
        "\n",
        "    storage_client = storage.Client() # create the Client.\n",
        "    bucket = storage_client.bucket(bucket_name) # get the bucket instance\n",
        "    blob = bucket.blob(source_file_name) # create a new blob\n",
        "\n",
        "    blob.download_to_filename(destination_blob_name) # upload the file\n",
        "\n",
        "    print(f\"File {source_file_name} downloaded to {destination_blob_name}.\") # print the success message\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "upload the 3 local csv files into our GCP cloud storage bucket."
      ],
      "metadata": {
        "id": "iVNIGDQQkszl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = \"6850test1\" # Replace with your bucket name\n",
        "source_file_name = \"customers.csv\" # Replace with the path to your local file\n",
        "destination_blob_name = \"customers.csv\" # Replace with the destination object name in the bucket\n",
        "\n",
        "upload_to_storage(bucket_name, source_file_name, destination_blob_name)"
      ],
      "metadata": {
        "id": "r7VkIhjkTLCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = \"6850test1\" # Replace with your bucket name\n",
        "source_file_name = \"orders.csv\" # Replace with the path to your local file\n",
        "destination_blob_name = \"orders.csv\" # Replace with the destination object name in the bucket\n",
        "\n",
        "upload_to_storage(bucket_name, source_file_name, destination_blob_name)"
      ],
      "metadata": {
        "id": "TM1pkJ0zTfQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = \"6850test1\" # Replace with your bucket name\n",
        "source_file_name = \"order_detail.csv\" # Replace with the path to your local file\n",
        "destination_blob_name = \"order_detail.csv\" # Replace with the destination object name in the bucket\n",
        "\n",
        "upload_to_storage(bucket_name, source_file_name, destination_blob_name)"
      ],
      "metadata": {
        "id": "JjKtFsWtWtQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now download them back from storage but this time don't save them to the filesystem, read them directly into a pandas dataframe in memory."
      ],
      "metadata": {
        "id": "ugscJ2JCkxG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gcsfs\n",
        "\n",
        "gcs = gcsfs.GCSFileSystem(project='fleet-space-407416')"
      ],
      "metadata": {
        "id": "W-uzt3voURpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = '6850test1'\n",
        "file_path = 'orders.csv'\n",
        "\n",
        "# Use the gcsfs file system object to open the CSV file\n",
        "with gcs.open(f'{bucket_name}/{file_path}') as file:\n",
        "    orders_df = pd.read_csv(file)\n"
      ],
      "metadata": {
        "id": "oo06KjCCULYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df.shape"
      ],
      "metadata": {
        "id": "1j1ZfMuYUdUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df.head()"
      ],
      "metadata": {
        "id": "tXiiIUflUeWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the gcsfs file system object to open the CSV file\n",
        "file_path = 'customers.csv'\n",
        "\n",
        "with gcs.open(f'{bucket_name}/{file_path}') as file:\n",
        "    customers_df = pd.read_csv(file)\n",
        "\n",
        "customers_df.shape"
      ],
      "metadata": {
        "id": "xzWEYQ7NUfwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customers_df.head()"
      ],
      "metadata": {
        "id": "lRuIqGFpVGBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "join the customers and orders pandas dataframes"
      ],
      "metadata": {
        "id": "nPDnhR6GlePv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers_orders = orders_df.merge(customers_df, left_on='CustomerID', right_on='CustomerID', how='left')\n",
        "customers_orders.columns"
      ],
      "metadata": {
        "id": "svWlyGYsU9iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customers_orders.head()"
      ],
      "metadata": {
        "id": "7ZFT4OybVBi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the gcsfs file system object to open the CSV file\n",
        "file_path = 'order_detail.csv'\n",
        "\n",
        "with gcs.open(f'{bucket_name}/{file_path}') as file:\n",
        "    order_detail_df = pd.read_csv(file)\n",
        "\n",
        "order_detail_df.shape"
      ],
      "metadata": {
        "id": "PpXiKgg_VzeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "order_detail_df.columns"
      ],
      "metadata": {
        "id": "VSZXXn3TXCps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "join the customers_order and order detail pandas dataframes"
      ],
      "metadata": {
        "id": "gXwjVq9Plj2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers_order_detail = customers_orders.merge(order_detail_df, left_on='OrderID', right_on='OrderID', how='left')\n",
        "customers_order_detail.columns"
      ],
      "metadata": {
        "id": "ZfHIOk7WXAc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "write the new dataframe to a parquet file"
      ],
      "metadata": {
        "id": "0w-r5sUflmTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers_order_detail.to_parquet('customers_order_detail.parquet')"
      ],
      "metadata": {
        "id": "2DRmwnBIXWmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "upload the parquet file to GCP cloud storage."
      ],
      "metadata": {
        "id": "NNfuLk9Ulocl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = \"6850test1\" # Replace with your bucket name\n",
        "source_file_name = \"customers_order_detail.parquet\" # Replace with the path to your local file\n",
        "destination_blob_name = \"customers_order_detail.parquet\" # Replace with the destination object name in the bucket\n",
        "\n",
        "upload_to_storage(bucket_name, source_file_name, destination_blob_name)"
      ],
      "metadata": {
        "id": "MI-0XrgPXVFX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}