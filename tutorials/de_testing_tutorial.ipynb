{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMRWzZwD60tMOuXbijvIHVS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewpecsok/data_engineering/blob/main/tutorials/de_testing_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to unit tests"
      ],
      "metadata": {
        "id": "I5xlK1gma8RX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "suppose we write a function intended to add two numbers together and return the result."
      ],
      "metadata": {
        "id": "x8By5EfoKNm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest"
      ],
      "metadata": {
        "id": "-eDpyDoxbj3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAz1F7PWJbI7"
      },
      "outputs": [],
      "source": [
        "def add_positive_numbers(a,b):\n",
        "  return a-b # broken intentionally"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "add_positive_numbers(6,7)"
      ],
      "metadata": {
        "id": "8lpapeInKWMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "how can we make sure this works correctly automatically?\n"
      ],
      "metadata": {
        "id": "Zy5uetyFLBvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Here we write a unit test to see if the function correctly returns the sum of the two numbers.\n",
        "\n",
        "Any functions that meet the following criteria will be run.\n",
        "\n",
        "*  within classes that inherit from TestCase\n",
        "*  names start with test_\n",
        "\n",
        "\n",
        "*This test should fail to pass.*"
      ],
      "metadata": {
        "id": "Y8dUl-wbMag9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the test\n",
        "\n",
        "We test to see if the function returns a result of 5.\n",
        "\n",
        "This is normally what you get when you add 2 and 3 together...."
      ],
      "metadata": {
        "id": "vBT47qAuboXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test should fail."
      ],
      "metadata": {
        "id": "zdRQ1NN3bRTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestAddNumbers(unittest.TestCase):\n",
        "    def test_add_positive_numbers(self):\n",
        "        result = add_positive_numbers(2, 3)\n",
        "        self.assertEqual(result, 5)  # Assert that the result is equal to 5"
      ],
      "metadata": {
        "id": "KwKnpyHObn6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the test."
      ],
      "metadata": {
        "id": "qEB-xBpDb6ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "id": "ud3hbN0XKgny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-1 is truly not equal to 5. This test caught the fact that the function incorrectly added two numbers together."
      ],
      "metadata": {
        "id": "SegDsumFbNyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we attempt to fix the function. We re-define the function and run the test again.\n",
        "\n",
        "This test should pass."
      ],
      "metadata": {
        "id": "IWMZd-VsMhjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixed function."
      ],
      "metadata": {
        "id": "l4dYtNjsbYzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_positive_numbers(a, b):\n",
        "    return a + b # fixed"
      ],
      "metadata": {
        "id": "2WuwFNx1Lpvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rerun Test."
      ],
      "metadata": {
        "id": "LE-0eZBlbdZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "id": "mEl8cC2PchOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Raising errors.\n",
        "\n",
        "Good error handling means raising errors in certain cases."
      ],
      "metadata": {
        "id": "diUWGEChbmXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we want to handle an error that SHOULD be raised?\n",
        "\n",
        "Here we add additional code that makes sure neither a or b is negative."
      ],
      "metadata": {
        "id": "kWMJanIFM-EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_positive_numbers(a, b):\n",
        "    if (a <= 0) or (b <= 0):\n",
        "        raise ValueError(\"Cannot pass negative numbers\")\n",
        "    return a + b"
      ],
      "metadata": {
        "id": "ZEAxVR0uclNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestNegative(unittest.TestCase):\n",
        "    def test_negative(self):\n",
        "        with self.assertRaises(ValueError):  # Asserts that a ValueError is raised\n",
        "            add_positive_numbers(-5, 10)"
      ],
      "metadata": {
        "id": "vphKAzXlM0yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "id": "wNQKf4Hec3Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding names to tests."
      ],
      "metadata": {
        "id": "C0ansYhrb_TE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we would like to know the names of the tests?\n",
        "\n",
        "We can override the addSuccess and addFailure methods.\n",
        "\n",
        "Overriding allows us to provide custom functionality that we choose to implement. In this case we make sure to call the base method that would normally run, and we print some text along with the test id.\n",
        "\n",
        "This code\n",
        "\n",
        "\n",
        "```\n",
        "print(f\"Test Passed: {test.id()}\")\n",
        "```\n",
        "and\n",
        "```\n",
        "print(f\"Test Failed: {test.id()}\")\n",
        "```\n",
        "\n",
        "is what gives us the test name/id"
      ],
      "metadata": {
        "id": "yP7iHXn8QF9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overriding addSuccess and addFailure"
      ],
      "metadata": {
        "id": "yP_0YvoacCmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class CustomTestResult(unittest.TestResult):\n",
        "    def addSuccess(self, test):\n",
        "        super().addSuccess(test)\n",
        "        print(f\"Test Passed: {test.id()}\")\n",
        "\n",
        "    def addFailure(self, test, err):\n",
        "        super().addFailure(test, err)\n",
        "        print(f\"Test Failed: {test.id()}\")"
      ],
      "metadata": {
        "id": "U4E0DQ_bPz2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we must then call our custom test class to make sure to use the functionality."
      ],
      "metadata": {
        "id": "dgsSaYlweXvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Updating the test run contructor."
      ],
      "metadata": {
        "id": "gdkHZv8OcHy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unittest.main(testRunner=unittest.TextTestRunner(resultclass=CustomTestResult),\n",
        "              argv=['first-arg-is-ignored'],\n",
        "              exit=False)"
      ],
      "metadata": {
        "id": "v7SdhSTedDa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating expressions"
      ],
      "metadata": {
        "id": "-SwAOFd6cPjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "there are many things we can check for\n",
        "\n",
        "what if we have an expression that should correctly check the length of a list and return either True or False?"
      ],
      "metadata": {
        "id": "lOn_P8UYSZwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array_length_five = [1,2,3,4,5]\n",
        "\n",
        "len(array_length_five) < 6"
      ],
      "metadata": {
        "id": "UJp7y-3gQYE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(array_length_five) < 4"
      ],
      "metadata": {
        "id": "Dc47VWfHSnuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LengthTest(unittest.TestCase):\n",
        "    def test_check_expression_true(self):\n",
        "        self.assertTrue(len([1,2,3,4,5]) < 10) # is this true is the length less than 10?\n"
      ],
      "metadata": {
        "id": "kIwb5Rfmex1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  unittest.main(testRunner=unittest.TextTestRunner(resultclass=CustomTestResult),\n",
        "                argv=['first-arg-is-ignored'],\n",
        "                exit=False)"
      ],
      "metadata": {
        "id": "boQKu-idUG3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how each time a new method is registered it is run by the unittest. Now we have 3 methods that have been run."
      ],
      "metadata": {
        "id": "jpJ2pNife0eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  unittest.main(testRunner=unittest.TextTestRunner(resultclass=CustomTestResult),\n",
        "                argv=['first-arg-is-ignored'],\n",
        "                exit=False)"
      ],
      "metadata": {
        "id": "Yx9T0PGlXpjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_dataframe():\n",
        "  import pandas as pd\n",
        "  df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n",
        "  return None"
      ],
      "metadata": {
        "id": "BaiTYnISu2H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataframes(unittest.TestCase):\n",
        "    def test_dataframes(self):\n",
        "        df = return_dataframe()\n",
        "        self.assertIsInstance(df, pd.DataFrame)  # Assert that the variable is a dataframe"
      ],
      "metadata": {
        "id": "RDEoz3OQuTES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  unittest.main(testRunner=unittest.TextTestRunner(resultclass=CustomTestResult),\n",
        "                argv=['first-arg-is-ignored'],\n",
        "                exit=False)"
      ],
      "metadata": {
        "id": "u0-xmGrRue3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Data Pipelines (multiple stages)"
      ],
      "metadata": {
        "id": "UyCXBDhgcUHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing in the context of Data Engineering.\n",
        "\n",
        "How to test a sequence of data transformations?\n",
        "\n",
        "Imagine we have order dates and delivery dates."
      ],
      "metadata": {
        "id": "f_HrqxSdfl5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformations as individual calls in sequence."
      ],
      "metadata": {
        "id": "j5Kbt3Nacf4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.internals.array_manager import NaT\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random as random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "order_dates = [\n",
        "    datetime(2023, 1, 1),\n",
        "    datetime(2023, 2, 15),\n",
        "    datetime(2023, 3, 8),\n",
        "    datetime(2023, 4, 22),\n",
        "    datetime(2023, 5, 10),\n",
        "    NaT,\n",
        "    datetime(2023, 7, 14),\n",
        "    datetime(2023, 8, 19),\n",
        "    datetime(2023, 9, 3),\n",
        "    datetime(2023, 10, 31)\n",
        "]\n",
        "\n",
        "received_dates = [\n",
        "    datetime(2023, 1, 13),\n",
        "    datetime(2023, 1, 25),\n",
        "    NaT,\n",
        "    datetime(2023, 5, 12),\n",
        "    datetime(2023, 6, 22),\n",
        "    datetime(2023, 8, 21),\n",
        "    datetime(2023, 8, 19),\n",
        "    datetime(2023, 9, 13),\n",
        "    datetime(2023, 10, 11),\n",
        "    datetime(2023, 12, 11)\n",
        "]\n",
        "\n",
        "dates_df_orig = pd.DataFrame({\n",
        "    'order_date' : order_dates,\n",
        "    'received_date' : received_dates\n",
        "})\n",
        "\n",
        "dates_df = dates_df_orig.copy()\n",
        "\n",
        "dates_df.head(3)"
      ],
      "metadata": {
        "id": "Ikd1yyh-fr8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "our first step to computing the number of days from order to delivery might be to drop any null values."
      ],
      "metadata": {
        "id": "i_HYdxgbia3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dates_df = dates_df.dropna().copy()\n",
        "dates_df"
      ],
      "metadata": {
        "id": "sGRM1hHQh-pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "our second step could then be to compute elapsed days"
      ],
      "metadata": {
        "id": "kVgM1KG_isPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dates_df.loc[:, 'elapsed_days'] = (dates_df['received_date'] - dates_df['order_date']).dt.days\n",
        "dates_df"
      ],
      "metadata": {
        "id": "VpNa87lJijw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "our final step might be then to drop impossible values (such as negative elapsed days)"
      ],
      "metadata": {
        "id": "vl5EyW7Htq04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dates_df = dates_df[dates_df['elapsed_days']> 0]\n",
        "dates_df"
      ],
      "metadata": {
        "id": "tRe5mr8ztol7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's imagine this is a production pipeline with each step a function.We'd like to run multiple commands in sequence and at each step check the results to be sure our transformations are correct.\n",
        "\n",
        "Let's create some helper functions."
      ],
      "metadata": {
        "id": "I7jsYoNiueg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using functions to create a pipeline."
      ],
      "metadata": {
        "id": "brWZuBj1clqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1\n",
        "def step_1_dropnull(df):\n",
        "  df = df.dropna().copy()\n",
        "  return df\n",
        "\n",
        "# Step 2\n",
        "def step_2_compute_days(df):\n",
        "  df.loc[:, 'elapsed_days'] = (df['received_date'] - df['order_date']).dt.days\n",
        "  return df\n",
        "\n",
        "# Step 3\n",
        "def step_3_dropnegative(df):\n",
        "  df = df[df['elapsed_days']> 0]\n",
        "  return df"
      ],
      "metadata": {
        "id": "EBVGEapdtyca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = step_1_dropnull(dates_df_orig)\n",
        "df = step_2_compute_days(df)\n",
        "df = step_3_dropnegative(df)\n",
        "df"
      ],
      "metadata": {
        "id": "6FdgPqDQc-u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a problem here though. Have we done ANY error handling? What happens if we pass bad data into our sequence?\n",
        "\n",
        "### Poor error handling.(really NO error handling)"
      ],
      "metadata": {
        "id": "EQP_n0kIdg4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = step_1_dropnull(\" Hi I'm NOT a dataframe. I'm a string! \")\n",
        "df = step_2_compute_days(df)\n",
        "df = step_3_dropnegative(df)\n",
        "df"
      ],
      "metadata": {
        "id": "5S_rOaQhdxfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good code and pipelines not only process data when everything is perfect. They handle situations in which the pipeline will fail and do so gracefully."
      ],
      "metadata": {
        "id": "l_BAJrP9d7z3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beginning to handle errors."
      ],
      "metadata": {
        "id": "8e20HwcneOka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def step_1_dropnull(df):\n",
        "  try:\n",
        "    if isinstance(df, pd.DataFrame):\n",
        "        # Process the DataFrame\n",
        "        df = df.dropna().copy()\n",
        "    else:\n",
        "        raise TypeError(\"Input is not a DataFrame.\")\n",
        "  except TypeError as e:\n",
        "    print(f\"step_1_dropnull: {e}\")\n",
        "    raise\n",
        "\n",
        "  return df\n",
        "\n",
        "def step_2_compute_days(df):\n",
        "  df.loc[:, 'elapsed_days'] = (df['received_date'] - df['order_date']).dt.days\n",
        "  return df\n",
        "\n",
        "def step_3_dropnegative(df):\n",
        "  df = df[df['elapsed_days']> 0]\n",
        "  return df"
      ],
      "metadata": {
        "id": "jfFWOQ_tcy-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = step_1_dropnull(\"hi i'm definitely not a dataframe\")\n",
        "df = step_2_compute_days(df)\n",
        "df = step_3_dropnegative(df)\n",
        "df"
      ],
      "metadata": {
        "id": "tevd-J0ewpy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is better, we've returned a custom error message that helps us diagnose the issue, but the code still crashes. We'd like to handle errors in such a way that the code doesn't crash, but rather fails gracefully."
      ],
      "metadata": {
        "id": "focN0KfCfCQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graceful error handling\n",
        "\n",
        "Here we stop returning an error and instead choose a number to represent failure. In this case we use -5 which means the passed data was not a dataframe. Now we can truly gracefully handle this error and can send a useful message to a data engineer to troubleshoot."
      ],
      "metadata": {
        "id": "FOOCipR8eX_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def the_pipeline(df):\n",
        "  try:\n",
        "    df = step_1_dropnull(df)\n",
        "    df = step_2_compute_days(df)\n",
        "    df = step_3_dropnegative(df)\n",
        "    df\n",
        "\n",
        "    return df\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Pipeline failed with error: {e}\")\n",
        "    return -5 # negative 5 means the pipeline failed."
      ],
      "metadata": {
        "id": "mk-MLkc7wtEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "the_pipeline(\"hi i'm definitely not a dataframe\")"
      ],
      "metadata": {
        "id": "nHP1xJzvxxnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test driven development\n",
        "\n",
        "Write the Test first. Then write the Code.\n",
        "\n",
        "This might seem like a meaningless change, but it causes you to think first about all the ways your code can and should fail."
      ],
      "metadata": {
        "id": "hX_SH3kVgK2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiplyNumbers(unittest.TestCase):\n",
        "    def test_multiply_two_numbers(self):\n",
        "        result = multiply_two_numbers(2, 4)\n",
        "        self.assertEqual(result, 8)  # Assert that the result is equal to 8"
      ],
      "metadata": {
        "id": "nMWlcmJ7gat0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiply_two_numbers(a,b):\n",
        "  return"
      ],
      "metadata": {
        "id": "mhfoyuFzhWTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  unittest.main(testRunner=unittest.TextTestRunner(resultclass=CustomTestResult),\n",
        "                argv=['first-arg-is-ignored'],\n",
        "                exit=False)"
      ],
      "metadata": {
        "id": "kxFhNxOvg5v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we know the test appears to work as it failed as expected.\n",
        "\n",
        "Refactor the code so the test will pass."
      ],
      "metadata": {
        "id": "X_edSOO8hklL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multiply_two_numbers(a,b):\n",
        "  result = a*b\n",
        "  return result"
      ],
      "metadata": {
        "id": "nqOtpDr3hjaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  unittest.main(testRunner=unittest.TextTestRunner(resultclass=CustomTestResult),\n",
        "                argv=['first-arg-is-ignored'],\n",
        "                exit=False)"
      ],
      "metadata": {
        "id": "sBowG5m-hjao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mocking\n",
        "\n"
      ],
      "metadata": {
        "id": "O-cJvpDLarbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def get_data_from_api():\n",
        "    response = requests.get('https://api.example.com/data')\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "QvwHUsVkc-u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "from unittest.mock import patch\n",
        "from unittest import mock\n",
        "import requests\n",
        "\n",
        "class TestGetDataFromAPI(unittest.TestCase):\n",
        "    @patch('requests.get')\n",
        "    def test_get_data_from_api(self, mock_get):\n",
        "        mock_response = {'data': 'mocked data'}\n",
        "        mock_get.return_value.json.return_value = mock_response\n",
        "\n",
        "        result = get_data_from_api()\n",
        "\n",
        "        mock_get.assert_called_once_with('https://api.example.com/data')\n",
        "        self.assertEqual(result, mock_response)\n",
        "\n",
        "    @patch('requests.get')\n",
        "    def test_successful_api_response(self, mock_get):\n",
        "        # Mock the API response\n",
        "        mock_response = mock.Mock()\n",
        "        mock_response.status_code = 200\n",
        "        mock_response.json.return_value = {'data': 'example'}\n",
        "        mock_get.return_value = mock_response\n",
        "\n",
        "        # Call the function that interacts with the API\n",
        "        result = get_data_from_api()\n",
        "\n",
        "        # Assertions\n",
        "        mock_get.assert_called_once_with('https://api.example.com/data')\n",
        "        self.assertEqual(result, {'data': 'example'})"
      ],
      "metadata": {
        "id": "MY1vteOuZ_kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  unittest.main(testRunner=unittest.TextTestRunner(resultclass=CustomTestResult),\n",
        "                argv=['first-arg-is-ignored'],\n",
        "                exit=False)"
      ],
      "metadata": {
        "id": "wlT9gxJ9cIBi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}